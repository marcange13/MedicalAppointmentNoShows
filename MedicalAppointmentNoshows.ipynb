{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25ca5f0-b101-449d-a591-0fe5228137bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before dropping:\n",
      " PatientId         0\n",
      "AppointmentID     0\n",
      "Gender            0\n",
      "ScheduledDay      0\n",
      "AppointmentDay    0\n",
      "Age               0\n",
      "Neighbourhood     0\n",
      "Scholarship       0\n",
      "Hipertension      0\n",
      "Diabetes          0\n",
      "Alcoholism        0\n",
      "Handcap           0\n",
      "SMS_received      0\n",
      "No-show           0\n",
      "dtype: int64\n",
      "Missing values after dropping:\n",
      " PatientId         0\n",
      "AppointmentID     0\n",
      "Gender            0\n",
      "ScheduledDay      0\n",
      "AppointmentDay    0\n",
      "Age               0\n",
      "Neighbourhood     0\n",
      "Scholarship       0\n",
      "Hipertension      0\n",
      "Diabetes          0\n",
      "Alcoholism        0\n",
      "Handcap           0\n",
      "SMS_received      0\n",
      "No-show           0\n",
      "dtype: int64\n",
      "Best criterion: gini with validation accuracy: 0.7946\n",
      "Decision Tree Test Accuracy: 0.7974\n",
      "Decision Tree Confusion Matrix:\n",
      " [[8775   53]\n",
      " [2186   39]]\n",
      "\n",
      "Random Forest with 10 estimators:\n",
      "Test Accuracy: 0.7957\n",
      "Confusion Matrix:\n",
      " [[8752   76]\n",
      " [2182   43]]\n",
      "\n",
      "Random Forest with 50 estimators:\n",
      "Test Accuracy: 0.7971\n",
      "Confusion Matrix:\n",
      " [[8766   62]\n",
      " [2181   44]]\n",
      "\n",
      "Random Forest with 100 estimators:\n",
      "Test Accuracy: 0.7972\n",
      "Confusion Matrix:\n",
      " [[8766   62]\n",
      " [2180   45]]\n",
      "\n",
      "Observations on Random Forest Performance:\n",
      "- With 10 estimators, accuracy was 0.7957\n",
      "- With 50 estimators, accuracy was 0.7971\n",
      "- With 100 estimators, accuracy was 0.7972\n",
      "Typically, increasing the number of estimators improves accuracy up to a point, then stabilizes or slightly decreases due to overfitting or noise.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Step 1: Reading the Dataset\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\marca\\Downloads\\KaggleV2-May-2016.csv\")\n",
    "\n",
    "# Check for missing values and drop them\n",
    "print(\"Missing values before dropping:\\n\", df.isnull().sum())\n",
    "df = df.dropna()  # Drop rows with any missing values (one-liner as hinted)\n",
    "print(\"Missing values after dropping:\\n\", df.isnull().sum())\n",
    "\n",
    "# Step 2: Feature Extraction\n",
    "# Extract specified features: Gender, Age, Scholarship, Hipertension, Diabetes, Alcoholism, Handcap, SMS_received\n",
    "# Target variable is 'No-show' (assuming this is the column indicating appointment attendance)\n",
    "features = ['Gender', 'Age', 'Scholarship', 'Hipertension', 'Diabetes', 'Alcoholism', 'Handcap', 'SMS_received']\n",
    "X = df[features].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "y = df['No-show']  # Target variable: 'Yes' (no-show) or 'No' (showed up)\n",
    "\n",
    "# Step 3: Preprocessing\n",
    "# a) Encoding categorical features (Gender and No-show are categorical)\n",
    "le = LabelEncoder()\n",
    "X['Gender'] = le.fit_transform(X['Gender'])  # Encode Gender: 'M' -> 0, 'F' -> 1\n",
    "\n",
    "# b) Scaling numeric features (Age is the only continuous numeric feature)\n",
    "scaler = StandardScaler()\n",
    "X['Age'] = scaler.fit_transform(X[['Age']])  # Scale Age (needs to be 2D for scaler)\n",
    "\n",
    "# c) Other features (Scholarship, Hipertension, etc.) are binary (0/1), so no scaling/encoding needed\n",
    "# d) NaN values already handled by dropna() in Step 2\n",
    "\n",
    "# Step 4: Splitting the Data\n",
    "# Split into 80% train, 10% validation, 10% test\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.1111, random_state=42)  # 0.1111 of 90% = 10% of total\n",
    "\n",
    "# Step 5: Training Tree-based Classifiers (Decision Tree)\n",
    "# Hyper-parameter tuning for criterion\n",
    "criteria = ['gini', 'entropy']  # Possible criteria for DecisionTreeClassifier\n",
    "val_scores = []\n",
    "\n",
    "for criterion in criteria:\n",
    "    dt = DecisionTreeClassifier(criterion=criterion, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_val_pred = dt.predict(X_val)\n",
    "    val_scores.append(accuracy_score(y_val, y_val_pred))\n",
    "\n",
    "# Choose the best criterion\n",
    "best_criterion = criteria[np.argmax(val_scores)]\n",
    "print(f\"Best criterion: {best_criterion} with validation accuracy: {max(val_scores):.4f}\")\n",
    "\n",
    "# Train final Decision Tree model with best criterion\n",
    "dt_final = DecisionTreeClassifier(criterion=best_criterion, random_state=42)\n",
    "dt_final.fit(X_train, y_train)\n",
    "\n",
    "# Classification Metrics for Decision Tree\n",
    "y_test_pred_dt = dt_final.predict(X_test)\n",
    "print(f\"Decision Tree Test Accuracy: {accuracy_score(y_test, y_test_pred_dt):.4f}\")\n",
    "print(\"Decision Tree Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred_dt))\n",
    "\n",
    "# Step 6: Random Forest\n",
    "# Train Random Forest with different numbers of estimators and compare\n",
    "n_estimators_list = [10, 50, 100]  # Test different numbers of trees\n",
    "rf_scores = []\n",
    "\n",
    "for n in n_estimators_list:\n",
    "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_test_pred_rf = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_test_pred_rf)\n",
    "    rf_scores.append(accuracy)\n",
    "    print(f\"\\nRandom Forest with {n} estimators:\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred_rf))\n",
    "\n",
    "# Comment on the difference in classification metrics\n",
    "print(\"\\nObservations on Random Forest Performance:\")\n",
    "for i, n in enumerate(n_estimators_list):\n",
    "    print(f\"- With {n} estimators, accuracy was {rf_scores[i]:.4f}\")\n",
    "print(\"Typically, increasing the number of estimators improves accuracy up to a point, then stabilizes or slightly decreases due to overfitting or noise.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7398700-4450-4cec-8c1b-1c2d9d60278e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
